{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wrapper - A3C.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOgoA9kLy++yuWZXex6OZb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mario2403/Unity-Python-Wrapper/blob/main/Wrapper_A3C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UecnwXEo-62O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class A3C:\n",
        "\n",
        "\n",
        "  def __init__(self, model, env, weights = None, logs_path = None, weights_path = None):\n",
        "\n",
        "    self.model = model\n",
        "    self.environment = env\n",
        "    self.behavior_name = list(self.environment.behavior_specs)[0]\n",
        "    self.spec = self.environment.behavior_specs[self.behavior_name]\n",
        "    self.weights = weights\n",
        "    self.logs_path = logs_path\n",
        "    self.weights_path = weights_path\n",
        "   \n",
        "\n",
        "\n",
        "  def train(self):\n",
        "    #==================== GLOBAL VARIABLES =====================\n",
        "    SEED = 22\n",
        "    EPISODES = 5000\n",
        "    STEPS = 64\n",
        "    VALUE_LOSS_COEF = 0.5\n",
        "    GAMMA = 0.99\n",
        "    MODEL_PATH = \"/content/modelo/unity_actor_critic.pth\"\n",
        "\n",
        "    #==================== MODEL =====================\n",
        "\n",
        "\n",
        "    self.model.train()\n",
        "\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "    tracked_agent=-1\n",
        "    save = 0\n",
        "\n",
        "    self.environment.reset()\n",
        "    decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "\n",
        "    print(f\"Actual tracked agent = {tracked_agent}\")\n",
        "    if tracked_agent == -1 and len(decision_steps) >= 1:\n",
        "          print(f\"agents = {len(decision_steps)}\")\n",
        "          tracked_agent = decision_steps.agent_id[0]\n",
        "          print(f\"New tracked agent = {tracked_agent}\")\n",
        "\n",
        "    decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "    obs = decision_steps[tracked_agent].obs\n",
        "    state = torch.from_numpy(np.array(decision_steps[tracked_agent].obs)).float()\n",
        "    total_values = []\n",
        "\n",
        "    for ep in range(int(EPISODES)):\n",
        "        self.environment.reset()\n",
        "        decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "        tracked_agent = -1 # -1 indicates not yet tracking\n",
        "        done = False\n",
        "\n",
        "        values = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "        while not done:\n",
        "            if tracked_agent == -1 and len(decision_steps) >= 1:\n",
        "              print(f\"agents = {len(decision_steps)}\")\n",
        "              tracked_agent = decision_steps.agent_id[0]\n",
        "\n",
        "            #print(f\"state = {state}\")\n",
        "            logits, value = self.model(state)\n",
        "\n",
        "            prob = F.softmax(logits, -1)\n",
        "\n",
        "            # Check if continuous or discrete\n",
        "            if self.spec.action_spec.continuous_size > 0:  \n",
        "              action = prob.multinomial(num_samples=self.spec.action_spec.continuous_size) # num samples serÃ¡ el numero de acciones\n",
        "            else:\n",
        "              action = prob.multinomial(num_samples=self.spec.action_spec.discrete_size)\n",
        "            #print(f\"raw action = {action}\")\n",
        "            log_prob = F.log_softmax(logits, -1)\n",
        "            #print(f\"log_prob = {log_prob}\")\n",
        "            log_prob = log_prob.gather(1, action)\n",
        "\n",
        "            action_unity = ActionTuple()\n",
        "            #print(f\"action = {action}\")\n",
        "\n",
        "            if self.spec.action_spec.continuous_size > 0: \n",
        "              action_unity.add_continuous(np.array(action))\n",
        "            else:\n",
        "              action_unity.add_discrete(np.array(action))\n",
        "\n",
        "            # Set the actions\n",
        "            # env.set_actions(behavior_name, action_unity)\n",
        "            self.environment.set_action_for_agent(self.behavior_name, tracked_agent, action_unity)\n",
        "            self.environment.step()\n",
        "\n",
        "            decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "            if tracked_agent in decision_steps:  #not done\n",
        "                obs = decision_steps[tracked_agent].obs\n",
        "                reward = decision_steps[tracked_agent].reward\n",
        "                done = False\n",
        "\n",
        "            if tracked_agent in terminal_steps:  # done\n",
        "                reward = terminal_steps[tracked_agent].reward\n",
        "                done = True\n",
        "\n",
        "            reward = np.clip(1, -1, reward)\n",
        "            #print(f\"calculated reward = {reward}\")\n",
        "            state = torch.from_numpy(np.array(obs)).float()\n",
        "\n",
        "            values.append(value)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            total_values.append(value.item())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        print(f\"Suma de recompensas = {np.sum(rewards)}, media de total_values =  {np.mean(total_values)}\")\n",
        "        print(\"=========== ACTUALIZANDO MODELO ===========\")\n",
        "        ###################################\n",
        "        ### Prepare for update the policy\n",
        "        ###################################\n",
        "\n",
        "        R = 0\n",
        "        if not done:\n",
        "            _, value = self.model(state)\n",
        "            R = value.data\n",
        "\n",
        "        values.append(R)\n",
        "        policy_loss = 0\n",
        "        value_loss = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = GAMMA * R + rewards[i]\n",
        "            advantage = R - values[i]\n",
        "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
        "            policy_loss = policy_loss - (log_probs[i] * Variable(advantage))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_fn = (policy_loss + VALUE_LOSS_COEF * value_loss)\n",
        "        #print(f\"loss_fn = {loss_fn}\")\n",
        "        loss_fn.sum().backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "        if save % 1000 == 0:\n",
        "            torch.save(self.model.state_dict(), MODEL_PATH)\n",
        "        save +=1\n",
        "        print(f\"Episodio = {ep}\")"
      ]
    }
  ]
}