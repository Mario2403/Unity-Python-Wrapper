{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unity_Wrapper_1.0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3F8pizx3ZREK6V5I1WeI2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mario2403/Unity-Python-Wrapper/blob/main/Unity_Wrapper_1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tpxB7YiGQjV",
        "outputId": "f747f7f1-ddc6-4248-935d-cfa18547b8a3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "%cp /gdrive/MyDrive/TFM/modelos_wrapper/wrapper_a3c.py ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMt180XcJhn4",
        "outputId": "577ef703-6067-4268-d2ed-7acc290b7ad7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "NuSsuR_5Rq3N",
        "outputId": "4ea0f33d-62be-43a2-a8db-b610b20cb56f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='100'\n",
              "            max='100',\n",
              "            style='width: 100%'\n",
              "        >\n",
              "            100\n",
              "        </progress>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting virtual X frame buffer: Xvfb.\n",
            "\u001b[K     |████████████████████████████████| 164 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 804.1 MB 2.8 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.8.1 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalled ml-agents\n"
          ]
        }
      ],
      "source": [
        "#@title Instalación de Dependencias { display-mode: \"form\" }\n",
        "#@markdown (Solo se necesitan en entornos alojados de Google Colab)\n",
        "\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))\n",
        "\n",
        "pro_bar = display(progress(0, 100), display_id=True)\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  INSTALL_XVFB = True\n",
        "except ImportError:\n",
        "  INSTALL_XVFB = 'COLAB_ALWAYS_INSTALL_XVFB' in os.environ\n",
        "\n",
        "if INSTALL_XVFB:\n",
        "  with open('frame-buffer', 'w') as writefile:\n",
        "    writefile.write(\"\"\"#taken from https://gist.github.com/jterrace/2911875\n",
        "XVFB=/usr/bin/Xvfb\n",
        "XVFBARGS=\":1 -screen 0 1024x768x24 -ac +extension GLX +render -noreset\"\n",
        "PIDFILE=./frame-buffer.pid\n",
        "case \"$1\" in\n",
        "  start)\n",
        "    echo -n \"Starting virtual X frame buffer: Xvfb\"\n",
        "    /sbin/start-stop-daemon --start --quiet --pidfile $PIDFILE --make-pidfile --background --exec $XVFB -- $XVFBARGS\n",
        "    echo \".\"\n",
        "    ;;\n",
        "  stop)\n",
        "    echo -n \"Stopping virtual X frame buffer: Xvfb\"\n",
        "    /sbin/start-stop-daemon --stop --quiet --pidfile $PIDFILE\n",
        "    rm $PIDFILE\n",
        "    echo \".\"\n",
        "    ;;\n",
        "  restart)\n",
        "    $0 stop\n",
        "    $0 start\n",
        "    ;;\n",
        "  *)\n",
        "        echo \"Usage: /etc/init.d/xvfb {start|stop|restart}\"\n",
        "        exit 1\n",
        "esac\n",
        "exit 0\n",
        "    \"\"\")\n",
        "  pro_bar.update(progress(5, 100))\n",
        "  !apt-get install daemon >/dev/null 2>&1\n",
        "  pro_bar.update(progress(10, 100))\n",
        "  !apt-get install wget >/dev/null 2>&1\n",
        "  pro_bar.update(progress(20, 100))\n",
        "  !wget http://security.ubuntu.com/ubuntu/pool/main/libx/libxfont/libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb >/dev/null 2>&1\n",
        "  pro_bar.update(progress(30, 100))\n",
        "  !wget --output-document xvfb.deb http://security.ubuntu.com/ubuntu/pool/universe/x/xorg-server/xvfb_1.18.4-0ubuntu0.12_amd64.deb >/dev/null 2>&1\n",
        "  pro_bar.update(progress(40, 100))\n",
        "  !dpkg -i libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb >/dev/null 2>&1\n",
        "  pro_bar.update(progress(50, 100))\n",
        "  !dpkg -i xvfb.deb >/dev/null 2>&1\n",
        "  pro_bar.update(progress(70, 100))\n",
        "  !rm libxfont1_1.5.1-1ubuntu0.16.04.4_amd64.deb\n",
        "  pro_bar.update(progress(80, 100))\n",
        "  !rm xvfb.deb\n",
        "  pro_bar.update(progress(90, 100))\n",
        "  !bash frame-buffer start\n",
        "  os.environ[\"DISPLAY\"] = \":1\"\n",
        "pro_bar.update(progress(100, 100))\n",
        "\n",
        "try:\n",
        "  import mlagents\n",
        "  print(\"ml-agents already installed\")\n",
        "except ImportError:\n",
        "  !python -m pip install -q mlagents==0.28.0\n",
        "  print(\"Installed ml-agents\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nueva sección"
      ],
      "metadata": {
        "id": "0dr8y3SGB7Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mlagents_envs.registry import default_registry\n",
        "from mlagents_envs.base_env import ActionSpec\n",
        "from mlagents_envs.environment import ActionTuple, BaseEnv\n",
        "\n",
        "#  AC LIBRARIES\n",
        "### Import libraries\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n"
      ],
      "metadata": {
        "id": "qC81dQHjVY-9"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "importlib.reload(wrapper_a3c)\n",
        "from wrapper_a3c import A3C"
      ],
      "metadata": {
        "id": "RZ2jyNqoLZ2Z"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnityWrapper:\n",
        "\n",
        "  available_envs = ['Basic', '3DBall', '3DBallHard', 'GridWorld', 'Hallway', 'VisualHallway', 'CrawlerDynamicTarget', 'CrawlerStaticTarget', 'Bouncer', 'SoccerTwos', 'PushBlock', 'VisualPushBlock', 'WallJump', 'Tennis', 'Reacher', 'Pyramids', 'VisualPyramids', 'Walker', 'FoodCollector', 'VisualFoodCollector', 'StrikersVsGoalie', 'WormStaticTarget', 'WormDynamicTarget']\n",
        "  environment = None\n",
        "\n",
        "  def __init__(self, env=None):\n",
        "    self.environment_name = env\n",
        "    self.set_environment(self.environment_name)\n",
        "    self.init_env()\n",
        "\n",
        "  def init_env(self):\n",
        "    self.environment.reset()\n",
        "    self.behavior_name = list(self.environment.behavior_specs)[0]\n",
        "    self.spec = self.environment.behavior_specs[self.behavior_name]\n",
        "    self.vis_obs = any(len(spec.shape) == 3 for spec in self.spec.observation_specs)\n",
        "\n",
        "  def info(self):\n",
        "    print(f\"Getting info of the selected environment...\")\n",
        "    if (self.environment_name == \"\" or self.environment_name == None) or (self.environment == None) or self.behavior_name == None:\n",
        "      print(\"[ERROR] You need to select an environment before asking for information, try using set_environment(env_name) function\")\n",
        "      return\n",
        "\n",
        "    print(\"\\n======= GENERAL INFO =======\")\n",
        "    print(f\"The selected environment is: {self.environment_name}\")\n",
        "    print(f\"Name of the behavior : {self.behavior_name}\")\n",
        "    print(f\"Number of observations : {len(self.spec.observation_specs)}\")\n",
        "    print(f\"Is there a visual observation ? {self.vis_obs}\")\n",
        "\n",
        "    print(\"\\n======= ACTION SPACE =======\")\n",
        "    if self.spec.action_spec.continuous_size > 0:\n",
        "      print(f\"There are {self.spec.action_spec.continuous_size} continuous actions\")\n",
        "    if self.spec.action_spec.is_discrete():\n",
        "      print(f\"There are {self.spec.action_spec.discrete_size} discrete actions\")\n",
        "\n",
        "    # For discrete actions only : How many different options does each action has ?\n",
        "    if self.spec.action_spec.discrete_size > 0:\n",
        "      for action, branch_size in enumerate(self.spec.action_spec.discrete_branches):\n",
        "        print(f\"Action number {action} has {branch_size} different options\")\n",
        "    print(\"\\n======= OBSERVATION INFO =======\")\n",
        "    self.environment.reset()\n",
        "\n",
        "    decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "\n",
        "    for index, obs_spec in enumerate(self.spec.observation_specs):\n",
        "      if len(obs_spec.shape) == 3:\n",
        "        print(\"Here is the first visual observation\")\n",
        "        plt.imshow(decision_steps.obs[index][0,:,:,:])\n",
        "        plt.show()\n",
        "\n",
        "    for index, obs_spec in enumerate(self.spec.observation_specs):\n",
        "      if len(obs_spec.shape) == 1:\n",
        "        print(f\"First vector observations : {decision_steps.obs[index][0,:]}\")\n",
        "        print(f\"Lenght of the observation = {len(decision_steps.obs[index][0,:])}\")\n",
        "\n",
        "\n",
        "\n",
        "  def close_env(self):\n",
        "    try:\n",
        "      self.environment.close()\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  def set_environment(self, environment_name):\n",
        "    self.close_env()\n",
        "    if environment_name in self.available_envs:\n",
        "      self.environment_name = environment_name\n",
        "      self.environment = default_registry[self.environment_name].make()\n",
        "      self.init_env()\n",
        "    else:\n",
        "      print(f\"The selected environment {environment_name} is not in the available environment list:\\n {self.available_envs}\")\n",
        "\n",
        "  def get_env(self):\n",
        "    return self.environment\n",
        "\n",
        "\n",
        "  def run_simulation(self):\n",
        "    print(\"STARTING SIMULATION\")\n",
        "    self.environment.reset()\n",
        "    for episode in range(3):\n",
        "      self.environment.reset()\n",
        "      decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "      tracked_agent = -1 # -1 indicates not yet tracking\n",
        "      done = False # For the tracked_agent\n",
        "      episode_rewards = 0 # For the tracked_agent\n",
        "      while not done:\n",
        "        # Track the first agent we see if not tracking\n",
        "        # Note : len(decision_steps) = [number of agents that requested a decision]\n",
        "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
        "          print(f\"agents = {len(decision_steps)}\")\n",
        "          tracked_agent = decision_steps.agent_id[0]\n",
        "          print(f\"actual agent = {tracked_agent}\")\n",
        "\n",
        "\n",
        "        # Generate an action for all agents\n",
        "        action = self.spec.action_spec.random_action(len(decision_steps))\n",
        "        # Set the actions\n",
        "        self.environment.set_actions(self.behavior_name, action)\n",
        "        #print(f\"action = {action.continuous}\")\n",
        "\n",
        "        # Move the simulation forward\n",
        "        self.environment.step()\n",
        "\n",
        "        # Get the new simulation results\n",
        "        decision_steps, terminal_steps = self.environment.get_steps(self.behavior_name)\n",
        "        if tracked_agent in decision_steps: # The agent requested a decision\n",
        "          episode_rewards += decision_steps[tracked_agent].reward\n",
        "        if tracked_agent in terminal_steps: # The agent terminated its episode\n",
        "          episode_rewards += terminal_steps[tracked_agent].reward\n",
        "          done = True\n",
        "      print(f\"Total rewards for episode {episode} is {episode_rewards}\")\n",
        "    print(\"END OF SIMULATION\")\n",
        "\n",
        "\n",
        "  def run_model(self, model):\n",
        "    if model == \"ActorCritic\":\n",
        "      self.run_AC()\n",
        "    else:\n",
        "      print(f\"The selected model {model} is not available. You can still implement your own model. For more info visit the official man page\")\n",
        "\n",
        "\n",
        "  def run_AC(self, weights = None, logs_path = None, weights_path = None):\n",
        "\n",
        "    class ActorCritic(nn.Module):\n",
        "      def __init__(self, n_inputs, n_outputs):\n",
        "          super(ActorCritic, self).__init__()\n",
        "          self.fc1 = nn.Linear(n_inputs, 24)\n",
        "          self.fc2 = nn.Linear(24, 36)\n",
        "          \n",
        "          self.actor = nn.Linear(36, n_outputs)\n",
        "          self.critic = nn.Linear(36, 1)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = F.relu(self.fc1(x))\n",
        "          x = F.relu(self.fc2(x))\n",
        "\n",
        "          policy = self.actor(x)\n",
        "          value = self.critic(x)\n",
        "\n",
        "          return policy, value\n",
        "    model = ActorCritic(self.spec.observation_specs[0].shape[0], self.spec.action_spec.discrete_branches[0]) # n_inputs , n_outputs\n",
        "\n",
        "    algorithm = A3C(model = model, env = self.environment, get_actiontuple = self.get_actiontuple, weights = weights, logs_path = logs_path, weights_path = weights_path)\n",
        "    algorithm.train()\n",
        "\n",
        "\n",
        "\n",
        "  def get_actiontuple(self, spec, action):\n",
        "    action_unity = ActionTuple()\n",
        "\n",
        "    if spec.action_spec.continuous_size > 0: \n",
        "      action_unity.add_continuous(np.array(action))\n",
        "    else:\n",
        "      action_unity.add_discrete(np.array(action))\n",
        "    return action_unity\n",
        "          \n",
        "\n"
      ],
      "metadata": {
        "id": "jRGB2rqTSE74"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper = UnityWrapper(\"Basic\")"
      ],
      "metadata": {
        "id": "oEQy5FzVSHtl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p--ma93xdC_N",
        "outputId": "623fd82d-5da0-4335-ad87-05328dd3462d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting info of the selected environment...\n",
            "======= GENERAL INFO =======\n",
            "The selected environment is: Basic\n",
            "Name of the behavior : Basic?team=0\n",
            "Number of observations : 1\n",
            "Is there a visual observation ? False\n",
            "======= ACTION SPACE =======\n",
            "There are 1 discrete actions\n",
            "Action number 0 has 3 different options\n",
            "======= OBSERVATION INFO =======\n",
            "First vector observations : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Lenght of the observation = 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.run_simulation()"
      ],
      "metadata": {
        "id": "9aGkTDvaj9Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.run_model(\"ActorCritic\")"
      ],
      "metadata": {
        "id": "RKIzv_X4pJjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.close_env()"
      ],
      "metadata": {
        "id": "ohximtAffGmk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrapper.close_env()\n",
        "wrapper = UnityWrapper(\"Basic\")\n",
        "wrapper.run_model(\"ActorCritic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "vc4Unwlt7Uc_",
        "outputId": "962be049-fa95-4411-cc3b-6a684590b10c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-b0114334cbf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Basic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ActorCritic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-83-344da4139ce8>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    116\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ActorCritic\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_AC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The selected model {model} is not available. You can still implement your own model. For more info visit the official man page\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-344da4139ce8>\u001b[0m in \u001b[0;36mrun_AC\u001b[0;34m(self, weights, logs_path, weights_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActorCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscrete_branches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# n_inputs , n_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA3C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_actiontuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actiontuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'get_actiontuple'"
          ]
        }
      ]
    }
  ]
}